{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment-analysis-imdb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZBX30qo4VxXVoeA0r16f9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avocadopelvis/imdb-movie-reviews-sentiment-analysis/blob/main/sentiment_analysis_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0OOfd4m3M0q7"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG_fV80jx4Yf"
      },
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "CW0qgfNAO7lh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "BYrD0DhVPHlB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufu3fWsOPHqZ",
        "outputId": "6fbce299-fc89-4af4-eae5-9feee0c1bd57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            "\r  0% 0.00/25.7M [00:00<?, ?B/s]\r 35% 9.00M/25.7M [00:00<00:00, 75.4MB/s]\n",
            "\r100% 25.7M/25.7M [00:00<00:00, 126MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip imdb-dataset-of-50k-movie-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1VlGhcKPHxB",
        "outputId": "67c701a3-d494-4152-8f4b-e71d4b8b6f67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re, string, unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob, Word\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "HQPP4qU_PH11"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset"
      ],
      "metadata": {
        "id": "2vWLIoZjkphW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_data = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "HgmHMc_1j4lf",
        "outputId": "885ee22e-eb0a-4ea3-8a8e-d420e7b10042"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "N1msjoO8l_9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "GNI8S9PKk5_O",
        "outputId": "5fc56fd5-4b20-4f44-dede-d33f51598364"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>50000</td>\n",
              "      <td>50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>49582</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Loved today's show!!! It was a variety and not...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review sentiment\n",
              "count                                               50000     50000\n",
              "unique                                              49582         2\n",
              "top     Loved today's show!!! It was a variety and not...  negative\n",
              "freq                                                    5     25000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Count"
      ],
      "metadata": {
        "id": "TX_wNJMamNkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_data['sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niHJstsumJSm",
        "outputId": "1c44de7c-bc0e-4ddf-9669-599d2679c8c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    25000\n",
              "positive    25000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We observe that there are equal number of negative and postive sentiment. Thus, the dataset is balanced."
      ],
      "metadata": {
        "id": "cQZyJJ-SmgtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the dataset"
      ],
      "metadata": {
        "id": "klGwWjkSmzSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train dataset\n",
        "train_reviews = imdb_data.review[:40000]\n",
        "train_sentiments = imdb_data.review[:40000]\n",
        "\n",
        "#test dataset\n",
        "test_reviews = imdb_data.review[40000:]\n",
        "test_sentiments = imdb_data.sentiment[40000:]\n",
        "\n",
        "print(train_reviews.shape, train_sentiments.shape)\n",
        "print(test_reviews.shape, test_sentiments.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqUzmhkWmcRd",
        "outputId": "bebc8aa8-ba05-43da-ced8-f714a2d493bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000,) (40000,)\n",
            "(10000,) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Normalization"
      ],
      "metadata": {
        "id": "E1D2bYUKn-Uc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Tokenization\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "#Setting English Stopwords\n",
        "nltk.download('stopwords')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nAYiy-Znr1z",
        "outputId": "18e5bcb1-41c0-4f36-f168-63db650e9e50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning the dataset"
      ],
      "metadata": {
        "id": "lWTEBfpOO5wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the html strips\n",
        "def strip_html(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  return soup.get_text()\n",
        "\n",
        "#removing the square brackets\n",
        "def remove_square(text):\n",
        "  return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "#removing special characters\n",
        "def remove_special(text, remove_digits=True):\n",
        "  pattern = r'[^a-zA-z0-9\\s]'\n",
        "  return re.sub(pattern, '', text)\n",
        "\n",
        "#removing the noisy text\n",
        "def remove_noise(text):\n",
        "  text = strip_html(text)\n",
        "  text = remove_square(text)\n",
        "  text = remove_special(text)\n",
        "  return text\n",
        "\n",
        "imdb_data['review'] = imdb_data['review'].apply(remove_noise)"
      ],
      "metadata": {
        "id": "p0dV_VDsoRRl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Stemming"
      ],
      "metadata": {
        "id": "NjEILvdBZ9Pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_stemmer(text):\n",
        "  ps = nltk.porter.PorterStemmer()\n",
        "  text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "  return text\n",
        "\n",
        "imdb_data['review'] = imdb_data['review'].apply(simple_stemmer)"
      ],
      "metadata": {
        "id": "zKY4A9F6ZuTh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords"
      ],
      "metadata": {
        "id": "G1IfDCmMcjwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set stopwords to english\n",
        "stop = set(stopwords.words('english'))\n",
        "print(stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-0YOUPPbyuH",
        "outputId": "eaf9f346-fe45-43db-ad05-da987f571a45"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'or', 'any', 'not', 'you', 'at', \"should've\", 'hasn', 'am', 've', \"won't\", 'above', 'hers', \"that'll\", 'weren', 'haven', 're', 'their', 'were', 'myself', 'them', 'out', 'as', 'needn', 'herself', 'a', 'again', 'shan', 'having', 'isn', 'your', 'most', 'where', 'he', 'off', 'all', 'did', \"you're\", \"you've\", 'yourself', 'can', 'its', \"isn't\", 'has', 'if', 'which', 'on', 'how', 'ourselves', 'through', 'about', 'mightn', \"haven't\", 'his', \"she's\", 'why', \"needn't\", 'same', 'theirs', 'for', \"couldn't\", 'will', 'during', 'themselves', \"you'll\", 'ma', 'hadn', 'me', 't', 'ain', 'the', 'while', \"mightn't\", 'was', 'm', 'between', 'those', 'under', 'below', 'should', 'other', 'yours', 'too', 'down', 'these', 'by', \"wasn't\", 'wasn', 's', 'what', 'him', 'is', 'aren', 'up', 'nor', \"mustn't\", 'are', 'shouldn', 'do', \"weren't\", 'once', 'didn', 'in', 'now', 'been', \"don't\", 'an', 'who', 'ours', 'own', 'i', \"doesn't\", \"shan't\", 'here', 'couldn', \"wouldn't\", 'until', 'to', 'this', \"hasn't\", 'll', 'it', 'have', 'some', 'than', 'she', 'does', 'y', 'against', 'before', 'with', 'won', 'after', 'then', 'they', 'my', 'over', 'because', 'her', 'more', 'itself', 'whom', 'be', 'such', 'each', 'that', 'both', 'our', 'doesn', 'being', 'had', \"shouldn't\", 'very', 'mustn', \"it's\", 'o', 'so', 'd', 'don', 'from', 'no', 'only', 'of', 'into', 'we', 'further', 'but', 'when', \"hadn't\", 'himself', 'just', 'yourselves', \"you'd\", 'doing', \"didn't\", 'few', 'wouldn', 'and', \"aren't\", 'there'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case = False):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  tokens = [token.strip() for token in tokens]\n",
        "  if is_lower_case:\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "  else:\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "  filtered_text = ' '.join(filtered_tokens)\n",
        "  return filtered_text\n",
        "\n",
        "imdb_data['review'] = imdb_data['review'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "OgqgilqGc3SG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalized Train Reviews"
      ],
      "metadata": {
        "id": "0R-ZEgvX3gQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_train_reviews = imdb_data.review[:40000]\n",
        "norm_train_reviews[0]"
      ],
      "metadata": {
        "id": "-WvkNSu1ggXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "2337bb05-0b9d-40d4-bd53-b6907cfc3e29"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one review ha mention watch 1 Oz episod youll hook right thi exactli happen meth first thing struck Oz wa brutal unflinch scene violenc set right word GO trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call OZ nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda Em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast Oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch Oz may becom comfort uncomfort viewingthat get touch darker side'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### As shown above, we can clearly observed that the stopwords such as have been removed while the text have been stemmed as well."
      ],
      "metadata": {
        "id": "QoI6juPI36O3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalized Test Reviews"
      ],
      "metadata": {
        "id": "m0mNzPwO4epN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_test_reviews = imdb_data.review[40000:]\n",
        "norm_test_reviews[40001]"
      ],
      "metadata": {
        "id": "-yPyhie8gnY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "e8d07e34-6520-4774-9d2e-3849b6225335"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wa excit see sitcom would hope repres indian candian found thi show funni produc cast probabl happi get bad good feed back becaus far concern get talk wa readi stereotyp problem becaus stereotyp exist reason usual true realli wasnt anyth funni stereotyp charact fresh boat dad doesnt understand hi daughter radic feminist muslim daughter way terribl actress young modern indian man tri run hi mosqu polit correct pretti good actor onli see get betterit veri contriv dialog doesnt flow well wa much potenti someth like thi sadli think fail dont realli care watch anoth episodei howev enjoy watch great canadian actress sheila mccarthi alway treat natur everyth doe bad daughter show doesnt act abil'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words Model"
      ],
      "metadata": {
        "id": "SKZ40vFW46NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count vectorizer\n",
        "cv = CountVectorizer(min_df = 0, max_df = 1, binary = False, ngram_range = (1, 3))\n",
        "\n",
        "#transformed train reviews\n",
        "cv_train_reviews = cv.fit_transform(norm_train_reviews)\n",
        "#transformed test reviews\n",
        "cv_test_reviews = cv.transform(norm_test_reviews)\n",
        "\n",
        "print('bow_cv_train:', cv_train_reviews.shape)\n",
        "print('bow_cv_test:', cv_test_reviews.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUDcgyIB42NW",
        "outputId": "cc45afee-e169-450b-aadc-feda5f150731"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bow_cv_train: (40000, 6209089)\n",
            "bow_cv_test: (10000, 6209089)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Term Frequency-Inverse Document Frequenct Model "
      ],
      "metadata": {
        "id": "M79YyEQ0_RdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf vectorizer\n",
        "tv = TfidfVectorizer(min_df = 0, max_df = 1, use_idf = True, ngram_range = (1, 3))\n",
        "\n",
        "#transformed train reviews\n",
        "tv_train_reviews = tv.fit_transform(norm_train_reviews)\n",
        "#tranformed test reviews\n",
        "tv_test_reviews = tv.transform(norm_test_reviews)\n",
        "\n",
        "print('tfidf_train:', tv_train_reviews.shape)\n",
        "print('tfidf_test:', tv_test_reviews.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDrEwFdl_IWx",
        "outputId": "858eff4f-ce5b-462e-8f40-7eef0ce144a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfidf_train: (40000, 6209089)\n",
            "tfidf_test: (10000, 6209089)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Labelling the sentiment text"
      ],
      "metadata": {
        "id": "ndPgPKxZC-Bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lb = LabelBinarizer()\n",
        "\n",
        "#transformed sentiment data\n",
        "sentiment_data = lb.fit_transform(imdb_data['sentiment'])\n",
        "print(sentiment_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWFoywa4Cyit",
        "outputId": "ea486138-569c-448a-b597-1d01c23692d0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the sentiment data"
      ],
      "metadata": {
        "id": "LfrkOYQvEAuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentiments = sentiment_data[:40000]\n",
        "test_sentiments = sentiment_data[40000:]"
      ],
      "metadata": {
        "id": "P6rmEsjqDz-X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_sentiments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MijwwX1EguN",
        "outputId": "5ec3af7d-cc72-43f0-926c-ad5eedfab8e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_sentiments)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce0M9noQEkIH",
        "outputId": "53f36948-a827-4f20-a611-1f4495e83cf2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " ...\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling the dataset"
      ],
      "metadata": {
        "id": "kL32diQAE0SS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "tUTipqOMePHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "lr = LogisticRegression(penalty = 'l2', max_iter = 500, C = 1, random_state = 42)\n",
        "\n",
        "#fitting the model for bag of words\n",
        "lr_bow = lr.fit(cv_train_reviews, train_sentiments)\n",
        "print(lr_bow)\n",
        "\n",
        "#fitting the model for tfidf features\n",
        "lr_tfidf = lr.fit(tv_train_reviews, train_sentiments)\n",
        "print(lr_tfidf)"
      ],
      "metadata": {
        "id": "7diFDYzXEmtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3dceb9-52a7-4599-b464-8825924db7e3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Model performance on test dataset"
      ],
      "metadata": {
        "id": "JJL_nSLWfw_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the model for bag of words\n",
        "lr_bow_predict = lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "\n",
        "#predicting the model for tfidf features\n",
        "lr_tfidf_predict = lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)"
      ],
      "metadata": {
        "id": "P5HvpxngfGUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2e3fd2-a89e-418c-a34b-04cfd58f63f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 1 1]\n",
            "[0 0 0 ... 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy of the model"
      ],
      "metadata": {
        "id": "z_oHtu_lg62Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy score for bag of words\n",
        "lr_bow_score = accuracy_score(test_sentiments, lr_bow_predict)\n",
        "print('lr_bow_score:', lr_bow_score)\n",
        "\n",
        "#accuracy score for tfidf features\n",
        "lr_tfidf_score = accuracy_score(test_sentiments, lr_tfidf_predict)\n",
        "print('lr_tfidf_score:', lr_tfidf_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eXiWRSFg3N2",
        "outputId": "063dc4f1-4427-44e1-c0d2-54c9ef977442"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr_bow_score: 0.7512\n",
            "lr_tfidf_score: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "Co8qEvwLi8Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#classification report for bag of words\n",
        "lr_bow_report = classification_report(test_sentiments, lr_bow_predict, target_names = ['Postive', 'Negative'])\n",
        "print(lr_bow_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pgQkJw7iumP",
        "outputId": "b43460b7-1cf7-49c5-cd51-996ab7eb699a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Postive       0.75      0.75      0.75      4993\n",
            "    Negative       0.75      0.75      0.75      5007\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.75      0.75      0.75     10000\n",
            "weighted avg       0.75      0.75      0.75     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#classification report for tfidf features\n",
        "lr_tfidf_report = classification_report(test_sentiments, lr_tfidf_predict, target_names= ['Postive', 'Negative'])\n",
        "print(lr_tfidf_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBugefzIjj2r",
        "outputId": "e8521dae-791d-4c14-a722-95123fd369f6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Postive       0.74      0.77      0.75      4993\n",
            "    Negative       0.76      0.73      0.75      5007\n",
            "\n",
            "    accuracy                           0.75     10000\n",
            "   macro avg       0.75      0.75      0.75     10000\n",
            "weighted avg       0.75      0.75      0.75     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GF-2F2sJmAy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}